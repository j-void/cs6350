{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-12-18T17:35:07.582513Z","iopub.status.busy":"2022-12-18T17:35:07.581819Z","iopub.status.idle":"2022-12-18T17:35:07.593643Z","shell.execute_reply":"2022-12-18T17:35:07.591988Z","shell.execute_reply.started":"2022-12-18T17:35:07.582464Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## load the data...\n","train_df = pd.read_csv('data/train_final.csv')\n","test_df = pd.read_csv('data/test_final.csv').iloc[:, 1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(train_df.columns)\n","#print(train_df.loc[train_df['workclass'] ==\"?\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T17:35:07.596653Z","iopub.status.busy":"2022-12-18T17:35:07.595489Z","iopub.status.idle":"2022-12-18T17:35:07.607294Z","shell.execute_reply":"2022-12-18T17:35:07.605764Z","shell.execute_reply.started":"2022-12-18T17:35:07.596593Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","class TDatasetTrain(Dataset):\n","    def __init__(self, path):\n","        train_df = pd.read_csv(path)\n","        X = train_df.iloc[:, :-1].copy()\n","        \n","        categorical_attrib = {\"workclass\": [\"Private\", \"Self-emp-not-inc\", \"Self-emp-inc\", \"Federal-gov\", \\\n","                                          \"Local-gov\", \"State-gov\", \"Without-pay\", \"Never-worked\"],\n","                             \"education\": [\"Bachelors\", \"Some-college\", \"11th\", \"HS-grad\", \"Prof-school\", \\\n","                                         \"Assoc-acdm\", \"Assoc-voc\", \"9th\", \"7th-8th\", \"12th\", \"Masters\", \\\n","                                         \"1st-4th\", \"10th\", \"Doctorate\", \"5th-6th\", \"Preschool\"],\n","                             \"marital.status\": [\"Married-civ-spouse\", \"Divorced\", \"Never-married\", \"Separated\",\\\n","                                              \"Widowed\", \"Married-spouse-absent\", \"Married-AF-spouse\"],\n","                             \"occupation\": [\"Tech-support\", \"Craft-repair\", \"Other-service\", \"Sales\", \"Exec-managerial\",\\\n","                                            \"Prof-specialty\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Adm-clerical\", \\\n","                                            \"Farming-fishing\", \"Transport-moving\", \"Priv-house-serv\", \"Protective-serv\",\\\n","                                            \"Armed-Forces\"],\n","                             \"relationship\": [\"Wife\", \"Own-child\", \"Husband\", \"Not-in-family\", \"Other-relative\",\\\n","                                              \"Unmarried\"],\n","                             \"race\": [\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"],\n","                             \"sex\": [\"Female\", \"Male\"],\n","                             \"native.country\":  [\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \\\n","                                              \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \\\n","                                              \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\",\\\n","                                              \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\",\\\n","                                              \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \\\n","                                              \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \\\n","                                              \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \\\n","                                              \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"]}\n","        \n","        self.max_id_dict = {}\n","        for key, value in categorical_attrib.items():\n","            for idx, arrtib in enumerate(value):\n","                X[key] = X[key].replace(arrtib, idx+1)\n","            max_id = X[key].value_counts().idxmax()\n","            self.max_id_dict[key] = max_id\n","            X[key] = X[key].replace(\"?\", max_id)\n","        X = pd.get_dummies(X)\n","        \n","        for key, value in categorical_attrib.items():\n","            for arrtib in value:\n","                if arrtib not in X.columns:\n","                    X[key+\"_\"+arrtib] = np.zeros((X.shape[0]))\n","        \n","        X = X.apply(pd.to_numeric)\n","        self.max_dict = {}\n","        max_dict = {}\n","        for attrib in X.columns:\n","            max_dict[attrib] = [max(X[attrib]), min(X[attrib])]\n","            max_, min_ = max_dict[attrib]\n","            range_ = max_ - min_\n","            range_ = 1 if range_ == 0 else range_\n","            X[attrib]  = (X[attrib]-min_)/range_\n","        self.max_dict = max_dict\n","\n","        \n","        self.X = X.values\n","        self.Y = train_df.iloc[:, -1].values\n","        \n","    def __len__(self):\n","        return len(self.Y)\n","    \n","    def __getitem__(self, idx):\n","        return torch.tensor(self.X[idx]).float(),  torch.tensor([self.Y[idx]]).float()\n","    \n","class TDatasetTest(Dataset):\n","    def __init__(self, path, max_list, max_id_dict):\n","        train_df = pd.read_csv(path)\n","        X = train_df.iloc[:, 1:].copy()\n","        #print(X.columns)\n","        categorical_attrib = {\"workclass\": [\"Private\", \"Self-emp-not-inc\", \"Self-emp-inc\", \"Federal-gov\", \\\n","                                          \"Local-gov\", \"State-gov\", \"Without-pay\", \"Never-worked\"],\n","                             \"education\": [\"Bachelors\", \"Some-college\", \"11th\", \"HS-grad\", \"Prof-school\", \\\n","                                         \"Assoc-acdm\", \"Assoc-voc\", \"9th\", \"7th-8th\", \"12th\", \"Masters\", \\\n","                                         \"1st-4th\", \"10th\", \"Doctorate\", \"5th-6th\", \"Preschool\"],\n","                             \"marital.status\": [\"Married-civ-spouse\", \"Divorced\", \"Never-married\", \"Separated\",\\\n","                                              \"Widowed\", \"Married-spouse-absent\", \"Married-AF-spouse\"],\n","                             \"occupation\": [\"Tech-support\", \"Craft-repair\", \"Other-service\", \"Sales\", \"Exec-managerial\",\\\n","                                            \"Prof-specialty\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Adm-clerical\", \\\n","                                            \"Farming-fishing\", \"Transport-moving\", \"Priv-house-serv\", \"Protective-serv\",\\\n","                                            \"Armed-Forces\"],\n","                             \"relationship\": [\"Wife\", \"Own-child\", \"Husband\", \"Not-in-family\", \"Other-relative\",\\\n","                                              \"Unmarried\"],\n","                             \"race\": [\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"],\n","                             \"sex\": [\"Female\", \"Male\"],\n","                             \"native.country\":  [\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \\\n","                                              \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \\\n","                                              \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\",\\\n","                                              \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\",\\\n","                                              \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \\\n","                                              \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \\\n","                                              \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \\\n","                                              \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"]}\n","        \n","        for key, value in categorical_attrib.items():\n","            for arrtib in value:\n","                if arrtib not in X.columns:\n","                    X[key+\"_\"+arrtib] = np.zeros((X.shape[0]))\n","                    \n","        for key, value in categorical_attrib.items():\n","            for idx, arrtib in enumerate(value):\n","                X[key] = X[key].replace(arrtib, idx+1)            \n","                X[key] = X[key].replace(\"?\", max_id_dict[key])\n","                    \n","        X = pd.get_dummies(X)\n","        #print(X.columns)\n","        X = X.apply(pd.to_numeric)\n","        #print(max_id_dict)\n","\n","            \n","            \n","        # X = X.apply(pd.to_numeric)\n","        for attrib in X.columns:\n","            max_, min_ = max_list[attrib]\n","            range_ = max_ - min_\n","            range_ = 1 if range_ == 0 else range_\n","            X[attrib]  = (X[attrib]-max_list[attrib][1])/range_\n","                    \n","        #print(X[\"native.country\"])\n","        self.X = X.values\n","\n","        self.Y = np.zeros((train_df.shape[0]))\n","        \n","    def __len__(self):\n","        return len(self.Y)\n","    \n","    def __getitem__(self, idx):\n","        return torch.tensor(self.X[idx]).float(),  torch.tensor([self.Y[idx]]).float()\n","\n","#test_data = TDatasetTest(\"data/test_final.csv\", train_data.max_dict, train_data.max_id_dict)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T19:09:44.369646Z","iopub.status.busy":"2022-12-18T19:09:44.369169Z","iopub.status.idle":"2022-12-18T19:09:44.388786Z","shell.execute_reply":"2022-12-18T19:09:44.387479Z","shell.execute_reply.started":"2022-12-18T19:09:44.369609Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","class LDatasetTrain(Dataset):\n","    def __init__(self, path, max_list=None):\n","        train_df = pd.read_csv(path)\n","        X = train_df.iloc[:, :-1].copy()\n","        categorical_attrib = {\"workclass\": [\"Private\", \"Self-emp-not-inc\", \"Self-emp-inc\", \"Federal-gov\", \\\n","                                          \"Local-gov\", \"State-gov\", \"Without-pay\", \"Never-worked\"],\n","                             \"education\": [\"Bachelors\", \"Some-college\", \"11th\", \"HS-grad\", \"Prof-school\", \\\n","                                         \"Assoc-acdm\", \"Assoc-voc\", \"9th\", \"7th-8th\", \"12th\", \"Masters\", \\\n","                                         \"1st-4th\", \"10th\", \"Doctorate\", \"5th-6th\", \"Preschool\"],\n","                             \"marital.status\": [\"Married-civ-spouse\", \"Divorced\", \"Never-married\", \"Separated\",\\\n","                                              \"Widowed\", \"Married-spouse-absent\", \"Married-AF-spouse\"],\n","                             \"occupation\": [\"Tech-support\", \"Craft-repair\", \"Other-service\", \"Sales\", \"Exec-managerial\",\\\n","                                            \"Prof-specialty\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Adm-clerical\", \\\n","                                            \"Farming-fishing\", \"Transport-moving\", \"Priv-house-serv\", \"Protective-serv\",\\\n","                                            \"Armed-Forces\"],\n","                             \"relationship\": [\"Wife\", \"Own-child\", \"Husband\", \"Not-in-family\", \"Other-relative\",\\\n","                                              \"Unmarried\"],\n","                             \"race\": [\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"],\n","                             \"sex\": [\"Female\", \"Male\"],\n","                             \"native.country\":  [\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \\\n","                                              \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \\\n","                                              \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\",\\\n","                                              \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\",\\\n","                                              \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \\\n","                                              \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \\\n","                                              \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \\\n","                                              \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"]}\n","        \n","        self.max_id_dict = {}\n","        for key, value in categorical_attrib.items():\n","            for idx, arrtib in enumerate(value):\n","                X[key] = X[key].replace(arrtib, idx+1)\n","            max_id = X[key].value_counts().idxmax()\n","            self.max_id_dict[key] = max_id\n","            X[key] = X[key].replace(\"?\", max_id)\n","            \n","            \n","        X = X.apply(pd.to_numeric)\n","        self.max_dict = {}\n","        max_dict = {}\n","        for attrib in X.columns:\n","            max_dict[attrib] = [max(X[attrib]), min(X[attrib])]\n","            max_, min_ = max_dict[attrib]\n","            #print(max_, min_)\n","            range_ = max_ - min_\n","            range_ = 1 if range_ == 0 else range_\n","            X[attrib]  = (X[attrib]-min_)/range_\n","        self.max_dict = max_dict\n","\n","                    \n","        self.X = X.values\n","        self.Y = train_df.iloc[:, -1].values\n","\n","        \n","    def __len__(self):\n","        return len(self.Y)\n","    \n","    def __getitem__(self, idx):\n","        return torch.tensor(self.X[idx]).float(),  torch.tensor([self.Y[idx]]).float()\n","    \n","from torch.utils.data import Dataset, DataLoader\n","class LDatasetTest(Dataset):\n","    def __init__(self, path, max_list, max_id_dict):\n","        train_df = pd.read_csv(path)\n","\n","        X = train_df.iloc[:, 1:].copy()\n","        categorical_attrib = {\"workclass\": [\"Private\", \"Self-emp-not-inc\", \"Self-emp-inc\", \"Federal-gov\", \\\n","                                          \"Local-gov\", \"State-gov\", \"Without-pay\", \"Never-worked\"],\n","                             \"education\": [\"Bachelors\", \"Some-college\", \"11th\", \"HS-grad\", \"Prof-school\", \\\n","                                         \"Assoc-acdm\", \"Assoc-voc\", \"9th\", \"7th-8th\", \"12th\", \"Masters\", \\\n","                                         \"1st-4th\", \"10th\", \"Doctorate\", \"5th-6th\", \"Preschool\"],\n","                             \"marital.status\": [\"Married-civ-spouse\", \"Divorced\", \"Never-married\", \"Separated\",\\\n","                                              \"Widowed\", \"Married-spouse-absent\", \"Married-AF-spouse\"],\n","                             \"occupation\": [\"Tech-support\", \"Craft-repair\", \"Other-service\", \"Sales\", \"Exec-managerial\",\\\n","                                            \"Prof-specialty\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Adm-clerical\", \\\n","                                            \"Farming-fishing\", \"Transport-moving\", \"Priv-house-serv\", \"Protective-serv\",\\\n","                                            \"Armed-Forces\"],\n","                             \"relationship\": [\"Wife\", \"Own-child\", \"Husband\", \"Not-in-family\", \"Other-relative\",\\\n","                                              \"Unmarried\"],\n","                             \"race\": [\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"],\n","                             \"sex\": [\"Female\", \"Male\"],\n","                             \"native.country\":  [\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \\\n","                                              \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \\\n","                                              \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\",\\\n","                                              \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\",\\\n","                                              \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \\\n","                                              \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \\\n","                                              \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \\\n","                                              \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"]}\n","        \n","        for key, value in categorical_attrib.items():\n","            for idx, arrtib in enumerate(value):\n","                X[key] = X[key].replace(arrtib, idx+1)            \n","            X[key] = X[key].replace(\"?\", max_id_dict[key])\n","            \n","            \n","        X = X.apply(pd.to_numeric)\n","        for attrib in X.columns:\n","            range_ = max_list[attrib][0]-max_list[attrib][1]\n","            range_ = 1 if range_ == 0 else range_\n","            X[attrib]  = (X[attrib]-max_list[attrib][1])/range_\n","                    \n","        #print(X[\"native.country\"])\n","        self.X = X.values\n","\n","        self.Y = np.zeros((train_df.shape[0]))\n","        \n","    def __len__(self):\n","        return len(self.Y)\n","    \n","    def __getitem__(self, idx):\n","        return torch.tensor(self.X[idx]).float(),  torch.tensor([self.Y[idx]]).float()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T17:44:40.385957Z","iopub.status.busy":"2022-12-18T17:44:40.385511Z","iopub.status.idle":"2022-12-18T17:44:40.404820Z","shell.execute_reply":"2022-12-18T17:44:40.403092Z","shell.execute_reply.started":"2022-12-18T17:44:40.385924Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T19:16:42.504235Z","iopub.status.busy":"2022-12-18T19:16:42.503817Z","iopub.status.idle":"2022-12-18T19:16:42.516124Z","shell.execute_reply":"2022-12-18T19:16:42.514765Z","shell.execute_reply.started":"2022-12-18T19:16:42.504203Z"},"trusted":true},"outputs":[],"source":["def weight_init(module, initf):\n","    def foo(m):\n","        classname = m.__class__.__name__.lower()\n","        if isinstance(m, module):\n","            initf(m.weight)\n","    return foo \n","\n","class NeuralNtwrk(nn.Module):\n","    def __init__(self, depth, width, ilen, activation=nn.ReLU(), init_wt=nn.init.xavier_normal_):\n","        super().__init__()\n","        self.layers = nn.ModuleList()\n","        self.activation_fn = activation\n","        self.initfn = init_wt\n","\n","        layer_zero = nn.Sequential(\n","                nn.Linear(ilen, width),\n","                self.activation_fn,\n","            )\n","        self.layers.append(layer_zero)\n","        \n","        for i in range(depth):\n","            layer = nn.Sequential(\n","                nn.Linear(width, width),\n","                self.activation_fn,\n","            )\n","            self.layers.append(layer)\n","        \n","        ## Final layer\n","        self.layers.append(nn.Linear(width, 1))\n","        \n","        self.apply(weight_init(module=nn.Linear, initf=self.initfn))\n","        \n","    \n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T19:48:49.562910Z","iopub.status.busy":"2022-12-18T19:48:49.562515Z","iopub.status.idle":"2022-12-18T20:36:24.344371Z","shell.execute_reply":"2022-12-18T20:36:24.343071Z","shell.execute_reply.started":"2022-12-18T19:48:49.562878Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","train_data = LDatasetTrain(\"data/train_final.csv\")\n","train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n","#print(train_data.max_dict)\n","test_data = LDatasetTest(\"data/test_final.csv\", train_data.max_dict, train_data.max_id_dict)\n","test_loader = DataLoader(test_data, batch_size=1)\n","\n","model = NeuralNtwrk(depth=5, width=40, ilen=train_data.X.shape[1], activation=nn.PReLU(), init_wt=nn.init.kaiming_normal_)\n","criterion = nn.MSELoss()\n","optim = torch.optim.Adam(model.parameters(), 1e-4, betas=(0.9, 0.999))\n","#optim = torch.optim.SGD(model.parameters(), 1e-4)\n","optim.zero_grad()\n","        \n","for epoch in tqdm(range(100), desc=\"Epochs: \"):\n","    loss_list = []\n","    for x, y in train_loader:\n","        optim.zero_grad()\n","        y_out = model(x)\n","        loss  = criterion(y_out, y)\n","        loss.backward()\n","        optim.step()\n","        loss_list.append(loss.detach().squeeze())\n","    tqdm.write('Epoch - '+str(epoch)+\": MSE=\"+str(np.mean(np.array(loss_list))))\n","    \n","tqdm.write('Running Test')\n","y_pred = []\n","y_pred_C = []\n","for x, y in test_loader:\n","    y_out = model(x).squeeze().detach().numpy()\n","    y_pred_C.append(y_out)\n","    y_out = 1 if y_out > 0.5 else 0 \n","    y_pred.append(y_out)\n","    \n","out_results = pd.DataFrame({\"ID\":np.arange(1, len(y_pred)+1, dtype=int),\"Prediction\":y_pred})\n","out_results.to_csv(\"submissions/submit_nn1_a.csv\", index=False)\n","out_results = pd.DataFrame({\"ID\":np.arange(1, len(y_pred_C)+1, dtype=int),\"Prediction\":y_pred_C})\n","out_results.to_csv(\"submissions/submit_nn1_b.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T21:22:37.511571Z","iopub.status.busy":"2022-12-18T21:22:37.511157Z","iopub.status.idle":"2022-12-18T22:20:15.003185Z","shell.execute_reply":"2022-12-18T22:20:15.002058Z","shell.execute_reply.started":"2022-12-18T21:22:37.511535Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","train_data = TDatasetTrain(\"data/train_final.csv\")\n","train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n","#print(train_data.max_dict)\n","test_data = TDatasetTest(\"data/test_final.csv\", train_data.max_dict, train_data.max_id_dict)\n","test_loader = DataLoader(test_data, batch_size=1)\n","\n","model = NeuralNtwrk(depth=3, width=120, ilen=train_data.X.shape[1], activation=nn.ReLU(), init_wt=nn.init.xavier_normal_)\n","criterion = nn.MSELoss()\n","optim = torch.optim.Adam(model.parameters(), 1e-4, betas=(0.9, 0.999))\n","optim.zero_grad()\n","        \n","for epoch in tqdm(range(100), desc=\"Epochs: \"):\n","    loss_list = []\n","    for x, y in train_loader:\n","        optim.zero_grad()\n","        y_out = model(x)\n","        loss  = criterion(y_out, y)\n","        loss.backward()\n","        optim.step()\n","        loss_list.append(loss.detach().squeeze())\n","    tqdm.write('Epoch - '+str(epoch)+\": MSE=\"+str(np.mean(np.array(loss_list))))\n","    \n","    \n","tqdm.write('Running Test')\n","y_pred = []\n","y_pred_C = []\n","for x, y in test_loader:\n","    y_out = model(x).squeeze().detach().numpy()\n","    y_pred_C.append(y_out)\n","    y_out = 1 if y_out > 0.5 else 0 \n","    y_pred.append(y_out)\n","    \n","out_results = pd.DataFrame({\"ID\":np.arange(1, len(y_pred)+1, dtype=int),\"Prediction\":y_pred})\n","out_results.to_csv(\"submissions/submit_nn2_a.csv\", index=False)\n","out_results = pd.DataFrame({\"ID\":np.arange(1, len(y_pred_C)+1, dtype=int),\"Prediction\":y_pred_C})\n","out_results.to_csv(\"submissions/submit_nn2_b.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T22:35:29.863509Z","iopub.status.busy":"2022-12-18T22:35:29.863118Z","iopub.status.idle":"2022-12-18T22:35:36.593948Z","shell.execute_reply":"2022-12-18T22:35:36.592640Z","shell.execute_reply.started":"2022-12-18T22:35:29.863479Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T20:55:32.134474Z","iopub.status.busy":"2022-12-18T20:55:32.134094Z","iopub.status.idle":"2022-12-18T20:56:54.945028Z","shell.execute_reply":"2022-12-18T20:56:54.943853Z","shell.execute_reply.started":"2022-12-18T20:55:32.134444Z"},"trusted":true},"outputs":[],"source":["## SVM Classification\n","train_df = pd.read_csv('data/train_final.csv')\n","test_df = pd.read_csv('data/test_final.csv').iloc[:, 1:]\n","train_df_new = train_df.iloc[:, :-1].copy()\n","train_df_new[\"age\"] = pd.cut(train_df_new[\"age\"],bins=[0,2,17,65,99], labels=[\"baby\",\"Child\",\"Adult\",\"Elderly\"])\n","train_df_new[\"fnlwgt\"] = pd.cut(train_df_new[\"fnlwgt\"],bins=[-1,np.median(train_df[\"fnlwgt\"]), np.max(train_df[\"fnlwgt\"])], labels=[\"0\",\"1\"])\n","train_df_new[\"education.num\"] = pd.cut(train_df_new[\"education.num\"],bins=[-1,np.median(train_df[\"education.num\"]), np.max(train_df[\"education.num\"])], labels=[\"0\",\"1\"])\n","train_df_new[\"capital.gain\"] = pd.cut(train_df_new[\"capital.gain\"],bins=[-1,1,np.max(train_df[\"capital.gain\"])], labels=[\"0\", \"1\"])\n","train_df_new[\"capital.loss\"] = pd.cut(train_df_new[\"capital.loss\"],bins=[-1,1, np.max(train_df[\"capital.loss\"])], labels=[\"0\", \"1\"])\n","train_df_new[\"hours.per.week\"] = pd.cut(train_df_new[\"hours.per.week\"],bins=[-1,20,40,60,np.max(train_df[\"hours.per.week\"])], labels=[\"part\",\"full\",\"over\",\"nosleep\"])\n","#print(train_df_new)\n","test_df_new = test_df.copy()\n","test_df_new[\"age\"] = pd.cut(test_df_new[\"age\"],bins=[0,2,17,65,99], labels=[\"baby\",\"Child\",\"Adult\",\"Elderly\"])\n","test_df_new[\"fnlwgt\"] = pd.cut(test_df_new[\"fnlwgt\"],bins=[-1,np.median(train_df[\"fnlwgt\"]), np.max(train_df[\"fnlwgt\"])], labels=[\"0\",\"1\"])\n","test_df_new[\"education.num\"] = pd.cut(test_df_new[\"education.num\"],bins=[-1,np.median(train_df[\"education.num\"]), np.max(train_df[\"education.num\"])], labels=[\"0\",\"1\"])\n","test_df_new[\"capital.gain\"] = pd.cut(test_df_new[\"capital.gain\"],bins=[-1,1,np.max(train_df[\"capital.gain\"])], labels=[\"0\", \"1\"])\n","test_df_new[\"capital.loss\"] = pd.cut(test_df_new[\"capital.loss\"],bins=[-1,1, np.max(train_df[\"capital.loss\"])], labels=[\"0\", \"1\"])\n","test_df_new[\"hours.per.week\"] = pd.cut(test_df_new[\"hours.per.week\"],bins=[-1,20,40,60,np.max(train_df[\"hours.per.week\"])], labels=[\"part\",\"full\",\"over\",\"nosleep\"])\n","#print(test_df_new)\n","\n","train_df_new_oh = pd.get_dummies(train_df_new)\n","test_df_new_oh = pd.get_dummies(test_df_new)\n","\n","native_country = [\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \\\n","                  \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \\\n","                  \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\",\\\n","                  \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\",\\\n","                  \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \\\n","                  \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \\\n","                  \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \\\n","                  \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"]\n","\n","for nc in native_country:\n","    if nc not in train_df_new_oh.columns:\n","        train_df_new_oh[\"native.country_\"+nc] = np.zeros((train_df_new_oh.shape[0]))\n","    if nc not in test_df_new_oh.columns:\n","        test_df_new_oh[\"native.country_\"+nc] = np.zeros((test_df_new_oh.shape[0]))\n","\n","X_train = train_df_new_oh.values\n","y_train = train_df.iloc[:, -1].values\n","X_test = test_df_new_oh.values\n","\n","from sklearn.svm import SVC\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","out_results = pd.DataFrame({\"ID\":np.arange(1, len(y_pred)+1, dtype=int),\"Prediction\":y_pred})\n","out_results.to_csv(\"submit_svc.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T21:20:12.363887Z","iopub.status.busy":"2022-12-18T21:20:12.363426Z","iopub.status.idle":"2022-12-18T21:22:11.980745Z","shell.execute_reply":"2022-12-18T21:22:11.979754Z","shell.execute_reply.started":"2022-12-18T21:20:12.363849Z"},"trusted":true},"outputs":[],"source":["## SVM Regression with max occurence for missing data\n","train_df = pd.read_csv('data/train_final.csv')\n","test_df = pd.read_csv('data/test_final.csv').iloc[:, 1:]\n","train_df_new = train_df.iloc[:, :-1].copy()\n","test_df_new = test_df.copy()\n","\n","categorical_attrib = {\"workclass\": [\"Private\", \"Self-emp-not-inc\", \"Self-emp-inc\", \"Federal-gov\", \\\n","                                          \"Local-gov\", \"State-gov\", \"Without-pay\", \"Never-worked\"],\n","                        \"education\": [\"Bachelors\", \"Some-college\", \"11th\", \"HS-grad\", \"Prof-school\", \\\n","                                    \"Assoc-acdm\", \"Assoc-voc\", \"9th\", \"7th-8th\", \"12th\", \"Masters\", \\\n","                                    \"1st-4th\", \"10th\", \"Doctorate\", \"5th-6th\", \"Preschool\"],\n","                        \"marital.status\": [\"Married-civ-spouse\", \"Divorced\", \"Never-married\", \"Separated\",\\\n","                                        \"Widowed\", \"Married-spouse-absent\", \"Married-AF-spouse\"],\n","                        \"occupation\": [\"Tech-support\", \"Craft-repair\", \"Other-service\", \"Sales\", \"Exec-managerial\",\\\n","                                    \"Prof-specialty\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Adm-clerical\", \\\n","                                    \"Farming-fishing\", \"Transport-moving\", \"Priv-house-serv\", \"Protective-serv\",\\\n","                                    \"Armed-Forces\"],\n","                        \"relationship\": [\"Wife\", \"Own-child\", \"Husband\", \"Not-in-family\", \"Other-relative\",\\\n","                                        \"Unmarried\"],\n","                        \"race\": [\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"],\n","                        \"sex\": [\"Female\", \"Male\"],\n","                        \"native.country\":  [\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \\\n","                                        \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \\\n","                                        \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\",\\\n","                                        \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\",\\\n","                                        \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \\\n","                                        \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \\\n","                                        \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \\\n","                                        \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"]}\n","\n","## Take care of missing attributes\n","for key, value in categorical_attrib.items():\n","    max_id = train_df_new[key].value_counts().idxmax()\n","    train_df_new[key] = train_df_new[key].replace(\"?\", max_id)\n","    test_df_new[key] = test_df_new[key].replace(\"?\", max_id)\n","\n","train_df_new_oh = pd.get_dummies(train_df_new)\n","test_df_new_oh = pd.get_dummies(test_df_new)\n","\n","for nc in categorical_attrib['native.country']:\n","    if nc not in train_df_new_oh.columns:\n","        train_df_new_oh[\"native.country_\"+nc] = np.zeros((train_df_new_oh.shape[0]))\n","    if nc not in test_df_new_oh.columns:\n","        test_df_new_oh[\"native.country_\"+nc] = np.zeros((test_df_new_oh.shape[0]))\n","\n","X_train = train_df_new_oh.values\n","y_train = train_df.iloc[:, -1].values\n","X_test = test_df_new_oh.values\n","\n","from sklearn.svm import SVR\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","regr = make_pipeline(StandardScaler(), SVR(C=2.0, epsilon=0.2))\n","regr.fit(X_train, y_train)\n","y_pred = regr.predict(X_test)\n","y_pred = np.where(y_pred<0.5,0,1)\n","out_results = pd.DataFrame({\"ID\":np.arange(1, len(y_pred)+1, dtype=int),\"Prediction\":y_pred})\n","out_results.to_csv(\"submissions/submit_svr1.csv\", index=False)\n","from sklearn.metrics import accuracy_score\n","y_pred_train = regr.predict(X_train)\n","y_pred_train = np.where(y_pred_train<0.5,0,1)\n","print(\"Train accuracy: \", accuracy_score(y_train, y_pred_train))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## SVM Regression with max occurence for specific output in missing data\n","train_df = pd.read_csv('data/train_final.csv')\n","test_df = pd.read_csv('data/test_final.csv').iloc[:, 1:]\n","\n","categorical_attrib = {\"workclass\": [\"Private\", \"Self-emp-not-inc\", \"Self-emp-inc\", \"Federal-gov\", \\\n","                                          \"Local-gov\", \"State-gov\", \"Without-pay\", \"Never-worked\"],\n","                        \"education\": [\"Bachelors\", \"Some-college\", \"11th\", \"HS-grad\", \"Prof-school\", \\\n","                                    \"Assoc-acdm\", \"Assoc-voc\", \"9th\", \"7th-8th\", \"12th\", \"Masters\", \\\n","                                    \"1st-4th\", \"10th\", \"Doctorate\", \"5th-6th\", \"Preschool\"],\n","                        \"marital.status\": [\"Married-civ-spouse\", \"Divorced\", \"Never-married\", \"Separated\",\\\n","                                        \"Widowed\", \"Married-spouse-absent\", \"Married-AF-spouse\"],\n","                        \"occupation\": [\"Tech-support\", \"Craft-repair\", \"Other-service\", \"Sales\", \"Exec-managerial\",\\\n","                                    \"Prof-specialty\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Adm-clerical\", \\\n","                                    \"Farming-fishing\", \"Transport-moving\", \"Priv-house-serv\", \"Protective-serv\",\\\n","                                    \"Armed-Forces\"],\n","                        \"relationship\": [\"Wife\", \"Own-child\", \"Husband\", \"Not-in-family\", \"Other-relative\",\\\n","                                        \"Unmarried\"],\n","                        \"race\": [\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"],\n","                        \"sex\": [\"Female\", \"Male\"],\n","                        \"native.country\":  [\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \\\n","                                        \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \\\n","                                        \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\",\\\n","                                        \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\",\\\n","                                        \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \\\n","                                        \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \\\n","                                        \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \\\n","                                        \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"]}\n","\n","## Take care of missing attributes\n","for index, row in train_df.iterrows():\n","    t_ = train_df.loc[train_df['income>50K'] == row['income>50K']]\n","    for key, value in categorical_attrib.items():\n","        max_id = t_[key].value_counts().idxmax()\n","        if train_df.at[index, key] == \"?\":\n","            train_df.at[index, key]=max_id \n","        if index < test_df.shape[0]:\n","            if test_df.at[index, key] == \"?\":\n","                test_df.at[index, key]=max_id \n","\n","train_df_new = train_df.iloc[:, :-1].copy()\n","test_df_new = test_df.copy()\n","train_df_new_oh = pd.get_dummies(train_df_new)\n","test_df_new_oh = pd.get_dummies(test_df_new)\n","\n","\n","for nc in categorical_attrib['native.country']:\n","    if nc not in train_df_new_oh.columns:\n","        train_df_new_oh[\"native.country_\"+nc] = np.zeros((train_df_new_oh.shape[0]))\n","    if nc not in test_df_new_oh.columns:\n","        test_df_new_oh[\"native.country_\"+nc] = np.zeros((test_df_new_oh.shape[0]))\n","        \n","max_dict = {}\n","for attrib in train_df_new_oh.columns:\n","    max_dict[attrib] = max(train_df_new_oh[attrib])\n","    max_dict[attrib] = 1 if max_dict[attrib] == 0 else max_dict[attrib]\n","    train_df_new_oh[attrib]  = train_df_new_oh[attrib]/max_dict[attrib]\n","    \n","for attrib in test_df_new_oh.columns:\n","    test_df_new_oh[attrib]  = test_df_new_oh[attrib]/max_dict[attrib]\n","\n","X_train = train_df_new_oh.values\n","y_train = train_df.iloc[:, -1].values\n","X_test = test_df_new_oh.values\n","\n","print(\"Preprocessing Done\")\n","from sklearn.svm import SVR\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","regr = make_pipeline(StandardScaler(), SVR(C=2.0, epsilon=0.2))\n","regr.fit(X_train, y_train)\n","y_pred = regr.predict(X_test)\n","y_pred = np.where(y_pred<=0.5,0,1)\n","out_results = pd.DataFrame({\"ID\":np.arange(1, len(y_pred)+1, dtype=int),\"Prediction\":y_pred})\n","out_results.to_csv(\"submissions/submit_svr2.csv\", index=False)\n","from sklearn.metrics import accuracy_score\n","y_pred_train = regr.predict(X_train)\n","y_pred_train = np.where(y_pred_train<=0.5,0,1)\n","print(\"Train accuracy: \", accuracy_score(y_train, y_pred_train))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## SVM Regression with all instances for missing data\n","train_df = pd.read_csv('data/train_final.csv')\n","test_df = pd.read_csv('data/test_final.csv').iloc[:, 1:]\n","\n","\n","categorical_attrib = {\"workclass\": [\"Private\", \"Self-emp-not-inc\", \"Self-emp-inc\", \"Federal-gov\", \\\n","                                          \"Local-gov\", \"State-gov\", \"Without-pay\", \"Never-worked\"],\n","                        \"education\": [\"Bachelors\", \"Some-college\", \"11th\", \"HS-grad\", \"Prof-school\", \\\n","                                    \"Assoc-acdm\", \"Assoc-voc\", \"9th\", \"7th-8th\", \"12th\", \"Masters\", \\\n","                                    \"1st-4th\", \"10th\", \"Doctorate\", \"5th-6th\", \"Preschool\"],\n","                        \"marital.status\": [\"Married-civ-spouse\", \"Divorced\", \"Never-married\", \"Separated\",\\\n","                                        \"Widowed\", \"Married-spouse-absent\", \"Married-AF-spouse\"],\n","                        \"occupation\": [\"Tech-support\", \"Craft-repair\", \"Other-service\", \"Sales\", \"Exec-managerial\",\\\n","                                    \"Prof-specialty\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Adm-clerical\", \\\n","                                    \"Farming-fishing\", \"Transport-moving\", \"Priv-house-serv\", \"Protective-serv\",\\\n","                                    \"Armed-Forces\"],\n","                        \"relationship\": [\"Wife\", \"Own-child\", \"Husband\", \"Not-in-family\", \"Other-relative\",\\\n","                                        \"Unmarried\"],\n","                        \"race\": [\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"],\n","                        \"sex\": [\"Female\", \"Male\"],\n","                        \"native.country\":  [\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \\\n","                                        \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \\\n","                                        \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\",\\\n","                                        \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\",\\\n","                                        \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \\\n","                                        \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \\\n","                                        \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \\\n","                                        \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"]}\n","\n","## Take care of missing attributes\n","for index, row in train_df.iterrows():\n","    row_ = row.copy()\n","    for key, value in categorical_attrib.items():\n","        if row[key] == '?':\n","            for v in value:\n","                row_[key] = v\n","                train_df.append(row_)\n","                \n","for key, value in categorical_attrib.items():   \n","   train_df.drop(train_df[train_df[key]=='?'].index, inplace=True)\n","            \n","for index, row in test_df.iterrows():\n","    t_ = train_df.loc[train_df['income>50K'] == row['income>50K']]\n","    for key, value in categorical_attrib.items():\n","        max_id = t_[key].value_counts().idxmax()\n","        if test_df.at[index, key] == \"?\":\n","            test_df.at[index, key]=max_id \n","\n","train_df_new = train_df.iloc[:, :-1].copy()\n","test_df_new = test_df.copy()\n","\n","train_df_new_oh = pd.get_dummies(train_df_new)\n","test_df_new_oh = pd.get_dummies(test_df_new)\n","\n","for nc in categorical_attrib['native.country']:\n","    if nc not in train_df_new_oh.columns:\n","        train_df_new_oh[\"native.country_\"+nc] = np.zeros((train_df_new_oh.shape[0]))\n","    if nc not in test_df_new_oh.columns:\n","        test_df_new_oh[\"native.country_\"+nc] = np.zeros((test_df_new_oh.shape[0]))\n","\n","X_train = train_df_new_oh.values\n","y_train = train_df.iloc[:, -1].values\n","X_test = test_df_new_oh.values\n","\n","print(\"Preprocessing Done\")\n","from sklearn.svm import SVR\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.1))\n","regr.fit(X_train, y_train)\n","y_pred = regr.predict(X_test)\n","y_pred = np.where(y_pred<=0.5,0,1)\n","out_results = pd.DataFrame({\"ID\":np.arange(1, len(y_pred)+1, dtype=int),\"Prediction\":y_pred})\n","out_results.to_csv(\"submissions/submit_svr3.csv\", index=False)\n","from sklearn.metrics import accuracy_score\n","y_pred_train = regr.predict(X_train)\n","y_pred_train = np.where(y_pred_train<=0.5,0,1)\n","print(\"Train accuracy: \", accuracy_score(y_train, y_pred_train))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T21:09:04.828234Z","iopub.status.busy":"2022-12-18T21:09:04.827795Z","iopub.status.idle":"2022-12-18T21:11:10.192650Z","shell.execute_reply":"2022-12-18T21:11:10.191828Z","shell.execute_reply.started":"2022-12-18T21:09:04.828201Z"},"trusted":true},"outputs":[],"source":["## SVM Regression\n","train_df = pd.read_csv('data/train_final.csv')\n","test_df = pd.read_csv('data/test_final.csv').iloc[:, 1:]\n","train_df_new = train_df.iloc[:, :-1].copy()\n","test_df_new = test_df.copy()\n","\n","train_df_new_oh = pd.get_dummies(train_df_new)\n","test_df_new_oh = pd.get_dummies(test_df_new)\n","\n","native_country = [\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \\\n","                  \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \\\n","                  \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\",\\\n","                  \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\",\\\n","                  \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \\\n","                  \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \\\n","                  \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \\\n","                  \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"]\n","\n","for nc in native_country:\n","    if nc not in train_df_new_oh.columns:\n","        train_df_new_oh[\"native.country_\"+nc] = np.zeros((train_df_new_oh.shape[0]))\n","    if nc not in test_df_new_oh.columns:\n","        test_df_new_oh[\"native.country_\"+nc] = np.zeros((test_df_new_oh.shape[0]))\n","\n","X_train = train_df_new_oh.values\n","y_train = train_df.iloc[:, -1].values\n","X_test = test_df_new_oh.values\n","\n","from sklearn.svm import SVR\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","regr = make_pipeline(StandardScaler(), SVR(kernel='poly', degree=5, gamma='auto', C=1.5, epsilon=0.3))\n","regr.fit(X_train, y_train)\n","y_pred = regr.predict(X_test)\n","y_pred = np.where(y_pred<0.5,0,1)\n","out_results = pd.DataFrame({\"ID\":np.arange(1, len(y_pred)+1, dtype=int),\"Prediction\":y_pred})\n","out_results.to_csv(\"submit_svr_poly.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"dl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"vscode":{"interpreter":{"hash":"777fa8a5189ce9b92aaea6da9d9f059d8f73b8ac5a7180998536790c2e871838"}}},"nbformat":4,"nbformat_minor":4}
